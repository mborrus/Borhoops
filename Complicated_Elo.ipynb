{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complicated Elo Predictor\n",
    "\n",
    "This is meant to copy the way that Nate Silver Built his, from his methodology here: https://www.natesilver.net/p/sbcb-methodology\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "import duckdb as db\n",
    "import warnings\n",
    "\n",
    "with open('config.yaml', 'r') as file:\n",
    "    config_file = yaml.safe_load(file)\n",
    "data_dir = config_file.get(\"data_dir\")\n",
    "output_dir = config_file.get(\"output_dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.read_csv(f'{data_dir}/Kaggle/SampleSubmissionStage2.csv')\n",
    "\n",
    "def extract_game_info(id_str):\n",
    "    # Extract year and team_ids\n",
    "    parts = id_str.split('_')\n",
    "    year = int(parts[0])\n",
    "    teamID1 = int(parts[1])\n",
    "    teamID2 = int(parts[2])\n",
    "    return year, teamID1, teamID2\n",
    "\n",
    "submission_df[['Season', 'TeamID1', 'TeamID2']] = submission_df['ID'].apply(extract_game_info).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some men's teams leave D1, so we *should* filter them out, but it breaks the basic model so I ignore it. The mean is still 1500.\n",
    "# mensids = db.sql('FROM \"./SourceData/Kaggle/MTeams.csv\" WHERE LastD1Season = 2025').to_df()\n",
    "mensids = db.sql('FROM \"./SourceData/Kaggle/MTeams.csv\"').to_df()\n",
    "\n",
    "womensids = db.sql('FROM \"./SourceData/Kaggle/WTeams.csv\" ').to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "womens_results = pd.read_csv(f'{data_dir}/Kaggle/WRegularSeasonCompactResults.csv')\n",
    "mens_results = pd.read_csv(f'{data_dir}/Kaggle/MRegularSeasonCompactResults.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point 1: Margin of victory: \n",
    "# Specifically, the margin of victory factor is calculated as (3 + s) ^ .85, where s is the scoring differential.\n",
    "# I assume this should be added to the change in elo for the winner/loser\n",
    "\n",
    "mens_results\n",
    "def point_differential_scaler(row, scaler=.85):\n",
    "    s = (row['WScore']- row['LScore'])\n",
    "    return (3+s) ** scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be a 10 point scaler to ELO\n",
    "# This could be improved by looking at the values of the losing teams, because as it is now it only uses the winning teams location\n",
    "# For calculating margins of victory, one point in a basketball game equals approximately 27 Elo points.\n",
    "\n",
    "def home_field_advantage_calculator(teams_df, season_cutoff = 2000, values_cutoff = 5):\n",
    "    # Filter mens_results to only include seasons from the year 2000 onwards\n",
    "    teams_df_filtered = teams_df[teams_df['Season'] >= season_cutoff]\n",
    "\n",
    "    # Calculate the average points scored by each team at home and away (including neutral courts)\n",
    "    home_points = teams_df_filtered[teams_df_filtered['WLoc'] == 'H'].groupby('WTeamID')['WScore'].mean().reset_index()\n",
    "    home_points.columns = ['TeamID', 'HomePoints']\n",
    "\n",
    "    away_points = teams_df_filtered[teams_df_filtered['WLoc'].isin(['A', 'N'])].groupby('WTeamID')['WScore'].mean().reset_index()\n",
    "    away_points.columns = ['TeamID', 'AwayPoints']\n",
    "\n",
    "    # Merge the home and away points dataframes\n",
    "    points_comparison = pd.merge(home_points, away_points, on='TeamID', how='inner')\n",
    "\n",
    "    # Calculate the expected additional points at home\n",
    "    points_comparison['HomeAdvantage'] = points_comparison['HomePoints'] - points_comparison['AwayPoints']\n",
    "    points_comparison['HomeAdvantage'] = points_comparison['HomeAdvantage'].clip(lower=-values_cutoff, upper=values_cutoff)\n",
    "\n",
    "    return points_comparison[['TeamID', 'HomeAdvantage']]\n",
    "\n",
    "HFA_df = home_field_advantage_calculator(mens_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Travel distance: 8 * m^(⅓) addded to the home court advantage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean reversion:\n",
    "# Empirically, the degree of mean reversion from year to year is growing — in other words, \n",
    "# teams are less likely to sustain their success — probably because the best players typically\n",
    "# leave for the NBA after one or two years in college; even elite programs now rarely maintain\n",
    "# dominance with the same core of talent. Currently, a team’s rating is reverted by 30-35 percent\n",
    "# toward the mean at the start of each new season.\n",
    "# Should revert to the mean of the conference ratings\n",
    "# The baysian model uses pre-season ratings, but this would be a bit harder to implement\n",
    "# \"partly on preseason rankings in the AP (media) and Coaches Polls.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K factor: Specifically, we use a k-factor of 38; this number has no intrinsic meaning and is derived empirically.\n",
    "# However, the k-factor is up to 50 percent higher (so, up to a k-factor of 56) for early-season games, \n",
    "# with this diminishing linearly to a k-factor of 38 until a team plays roughly the 20th game of its season.\n",
    "\n",
    "def k_factor_calculator(game_number, k_factor_start=56, k_factor_end = 38):\n",
    "    \"\"\" takes the input of the game number and returns the k-factor for the game\n",
    "    :param game_number: int, the number of the game\n",
    "    :param k_factor_start: int, the starting k-factor for the first game\n",
    "    :param k_factor_end: int, the ending k-factor for the last game\n",
    "    :return: int, the k-factor for the game\n",
    "    \"\"\"\n",
    "    if k_factor > k_factor_end:\n",
    "        k_factor = k_factor_start - (game_number - 1)\n",
    "    else:\n",
    "        k_factor = 38\n",
    "    return k_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NCAA tournament games also receive an additional multiple of 1.25x, tantamount to a k-factor of 47.5\n",
    "# Not sure if I want to look at NCAA games as well, but maybe\n",
    "# An additional multiplier of 1.07x is applied to the Elo ratings\n",
    "# difference between the teams in forecasting margins of victory and win probabilities in the tournament. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WOMEN:\n",
    "# less mean-reversion from season to season\n",
    "# Home court advantage tends to be slightly less in the women’s game\n",
    "# the ratio of Elo rating point differences to the point spread is about 25:1 for women as opposed to 27:1 for men"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1495644420.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[67], line 6\u001b[0;36m\u001b[0m\n\u001b[0;31m    Womens:\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Composite with https://kenpom.com/? (1.5x)\n",
    "# Composite with https://sonnymoorepowerratings.com/m-basket.htm?\n",
    "# https://www.espn.com/mens-college-basketball/bpi? \n",
    "# Massey ratings: https://masseyratings.com/cb2024/ncaad1/ratings\n",
    "\n",
    "# Womens:\n",
    "# https://herhoopstats.com/stats/ncaa/research/team_single_seasons/?min_season=2025&max_season=2025&division=1&games=all&criteria0=hhs_net_rtg&comp0=ge&threshold0=-100&stats_to_show=summary_advanced&submit=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_set = 30 # K-factor for Elo rating\n",
    "# K-factor determines how much the Elo rating changes after each game\n",
    "# Higher K-factor means more volatility in Elo ratings, Lower K-factor means more stable Elo ratings\n",
    "# The K-factor is usually set between 10 and 40, 30 being the standard\n",
    "initial_elo_set = 1500 # Initial Elo rating for all teams\n",
    "# Initial Elo rating is usually set to 1500, but can be set to any value\n",
    "mean_reversion = .25 # Mean reversion ratio for Elo rating\n",
    "# Mean reversion ratio determines how much the Elo rating reverts to the initial/mean Elo rating after each season\n",
    "# Used to reflect the turnover in a sports team, higher mean reversion means more turnover in the team\n",
    "# I chose to go with 25% of the elo returns to mean as a starting point\n",
    "\n",
    "def update_elo(winner_elo, loser_elo, k=k_set):\n",
    "    expected_win = 1 / (1 + 10**((loser_elo - winner_elo) / 400))\n",
    "    new_winner_elo = winner_elo + k * (1 - expected_win)\n",
    "    new_loser_elo = loser_elo - k * (1 - expected_win)\n",
    "    return new_winner_elo, new_loser_elo\n",
    "\n",
    "def run_basic_elo(season_results_df, ids_df):\n",
    "\n",
    "    seasons_array = sorted(season_results_df['Season'].unique())\n",
    "    initial_elo = initial_elo_set \n",
    "    elo_ratings = {team_id: initial_elo for team_id in ids_df['TeamID'].unique()}\n",
    "\n",
    "    for i in seasons_array:\n",
    "        results_season = season_results_df[season_results_df['Season'] == i]\n",
    "        # print(i)\n",
    "        for index, row in results_season.iterrows():\n",
    "            winner = row['WTeamID']\n",
    "            loser = row['LTeamID']\n",
    "            if row['WLoc'] == 'H':\n",
    "                winner_elo = elo_ratings[winner] + 100\n",
    "            elif row['WLoc'] == 'A':\n",
    "                loser_elo = elo_ratings[loser] + 100\n",
    "                \n",
    "            winner_elo = elo_ratings[winner]\n",
    "            loser_elo = elo_ratings[loser]\n",
    "            new_winner_elo, new_loser_elo = update_elo(winner_elo, loser_elo)\n",
    "            elo_ratings[winner] = new_winner_elo\n",
    "            elo_ratings[loser] = new_loser_elo\n",
    "        elo_ratings = {team_id: (1-mean_reversion) * elo + (mean_reversion) * initial_elo for team_id, elo in elo_ratings.items()}\n",
    "        df = pd.DataFrame(list(elo_ratings.items()), columns=['TeamID', 'Elo'])\n",
    "\n",
    "    return df\n",
    "\n",
    "mens_elo = run_basic_elo(mens_results, mensids)\n",
    "womens_elo = run_basic_elo(womens_results, womensids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mens_elo' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m All_elo \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([\u001b[43mmens_elo\u001b[49m, womens_elo], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Create a dictionary for quick lookup of ELO ratings by TeamID\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mens_elo' is not defined"
     ]
    }
   ],
   "source": [
    "All_elo = pd.concat([mens_elo, womens_elo], ignore_index=True)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "# Create a dictionary for quick lookup of ELO ratings by TeamID\n",
    "elo_dict = All_elo.set_index('TeamID')['Elo'].to_dict()\n",
    "\n",
    "# Map the ELO ratings to the TeamID1 column in the submission_df\n",
    "submission_df['TeamID1_Elo'] = submission_df['TeamID1'].map(elo_dict)\n",
    "submission_df['TeamID2_Elo'] = submission_df['TeamID2'].map(elo_dict)\n",
    "\n",
    "# Fill missing values with 9999 - these would be teams that aren't in the nate database of mismatches in names\n",
    "submission_df['TeamID1_Elo'].fillna(9999, inplace=True)\n",
    "submission_df['TeamID2_Elo'].fillna(9999, inplace=True)\n",
    "\n",
    "# Check the result, this should be 0\n",
    "assert len(submission_df.query('TeamID1_Elo == 9999 or TeamID2_Elo == 9999')) == 0, \"There are teams with missing ELO ratings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic ELO win probability calculation\n",
    "def calc_elo_win(A, B):\n",
    "    awin = 1 / (1 + 10**( (B - A) / 400))\n",
    "    return(awin)\n",
    "submission_df['Team1_win_prob'] = submission_df.apply(lambda x: calc_elo_win(x['TeamID1_Elo'], x['TeamID2_Elo']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Output = submission_df[['ID', 'Team1_win_prob']].rename(columns={'Team1_win_prob': 'Pred'})\n",
    "Output.to_csv(f'{output_dir}/BasicEloProbs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ballenvy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
